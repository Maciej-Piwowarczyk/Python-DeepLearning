# -*- coding: utf-8 -*-
"""01_MNIST w Dense

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fib9l-Z2ET8CcsGjB7Q_C_FnNspTWeEk

Goal: Specify a network with `tf.keras.layers`, train it on the MNIST dataset, and try out 2 or 3 variations of different architectures. I.e., change the number of neurons or layers, change the activation function (you can find more in the documentation at [`tf.nn`](https://www.tensorflow.org/api_docs/python/tf/nn)), or even change the optimizer ([`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)).
"""

# We'll start with our library imports...
import numpy as np                 # to use numpy arrays
import tensorflow as tf            # to specify and run computation graphs
import tensorflow_datasets as tfds # to load training data
import matplotlib.pyplot as plt    # to visualize data and draw plots
from tqdm import tqdm              # to track progress of loops

DATA_DIR = './tensorflow-datasets/'

# Downloading and defining the train, validation, and test sets
# Using the Cifar-10 MNIST dataset
(ds_train, ds_val, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train[:80%]', 'train[80%:]', 'test'], # 40k train, 20k val, 10k test
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

# Normalize the data
def preprocess(image, label):
    image = tf.reshape(tf.cast(image, tf.float32) / 255.0, [-1, 784])
    return image, label

train_ds = ds_train.map(preprocess).batch(32)
val_ds = ds_val.map(preprocess).batch(32)
test_ds = ds_test.map(preprocess).batch(32)

# using Sequential groups all the layers to run at once
model1 = tf.keras.Sequential()
model1.add(tf.keras.layers.Dense(100, tf.nn.relu))
model1.add(tf.keras.layers.Dense(50, tf.nn.relu))
model1.add(tf.keras.layers.Dense(10))

model2 = tf.keras.Sequential()
model2.add(tf.keras.layers.Dense(100, tf.nn.relu))
model2.add(tf.keras.layers.Dense(10))

# Model Compilation
model1.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)
model2.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Model Training
One_history = model1.fit(
    train_ds,
    epochs=100,
    validation_data=val_ds,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=8
        )
    ],
)

print("\n===== NEXT MODEL =====\n")

Two_history = model2.fit(
    train_ds,
    epochs=100,
    validation_data=val_ds,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=8
        )
    ],
)

models = [model1, model2]
histories = [One_history, Two_history]

def plot_model(model_history, title):
  # Plot training history
  plt.figure(figsize=(12, 6))

  # Plot training & validation accuracy values
  plt.subplot(1, 2, 1)
  plt.plot(model_history.history['accuracy'], label='Training Accuracy')
  plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')
  plt.title(f'Training and Validation Accuracy for {title}')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()

  # Plot training & validation loss values
  plt.subplot(1, 2, 2)
  plt.plot(model_history.history['loss'], label='Training Loss')
  plt.plot(model_history.history['val_loss'], label='Validation Loss')
  plt.title('Training and Validation Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

  plt.tight_layout()
  plt.show()

for idx, history in enumerate(histories):
  plot_model(history, title=f"Model {idx+1}")

test_accuracies = [model.evaluate(test_ds)[1] for model in models]
print()
for idx, test_accuracy in enumerate(test_accuracies):
  print(f'Test accuracy (Model {idx+1}):', test_accuracy)