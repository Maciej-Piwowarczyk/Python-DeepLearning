# -*- coding: utf-8 -*-
"""03_IMDB Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJHie7tKvC7ZsRX6HUSnbYedhdhmibfK

For this assignment, you will apply sequential neural network models to the problem of sentiment analysis on the IMDB dataset Links to an external site., which consists of 50,000 reviews labeled as positive or negative. (Also see the Sentiment Analysis Links page at NLP Progress and the TensorFlow documentation.)

Design and implement at least two sequential architectures for this problem. These could be based on LSTMs, GRUs, or transformers.  (It is okay to use, e.g., LSTMs for both, if you vary structure.) For each architecture you should use an attention mechanism and a regularizer, and try at least two settings of hyperparameter values.  Thus you will evaluate at least four models.  

You should employ early stopping in your training, with a minimum patience hyperparameter value of 5.  This means that training is not to end until you see no improvement in your objective function for at least 5 training epochs.
"""

# Imports
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
from pathlib import Path

(ds_train, ds_val, ds_test), ds_info = tfds.load(
    'imdb_reviews',
     split=['train[:90%]', 'train[90%:]', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True
    )

train_ds =  (ds_train
             .map(lambda _, label: (_, tf.cast(label, tf.float32)))
             .shuffle(len(ds_train))
             .batch(64)
)
val_ds = (ds_val
          .map(lambda _, label: (_, tf.cast(label, tf.float32)))
          .shuffle(len(ds_val))
          .batch(64)
)
test_ds =  (ds_test
             .map(lambda _, label: (_, tf.cast(label, tf.float32)))
             .shuffle(len(ds_test))
             .batch(64)
)

MAX_TOKENS = 3_000
MAX_SEQUENCE_LENGTH = 128
EMBEDDING_SIZE = 1_000

# Vector Layer Initialization
vect_layer = tf.keras.layers.TextVectorization(MAX_TOKENS, output_sequence_length=MAX_SEQUENCE_LENGTH)
vect_layer.adapt(ds_train.map(lambda x, y: x))
vect_layer.trainable = False

# Custom Models

@tf.keras.utils.register_keras_serializable()
class Arch1(tf.keras.Model):
  def __init__(self, vect_layer):
    super().__init__()
    self.vect_layer = vect_layer
    self.embedding = tf.keras.layers.Embedding(len(vect_layer.get_vocabulary()), EMBEDDING_SIZE, mask_zero=True)
    self.bidirectional_1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True))
    self.query = tf.keras.layers.Conv1D(200, 4, padding='same')
    self.value = tf.keras.layers.Conv1D(200, 4, padding='same')
    self.attention = tf.keras.layers.Attention(dropout=0.6)
    self.bidirectional_2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64))
    self.dropout = tf.keras.layers.Dropout(0.6)
    self.dense = tf.keras.layers.Dense(1, activation='sigmoid')

  def get_config(self):
    base_config = super().get_config()
    config =  {
        "vect_layer": tf.keras.utils.serialize_keras_object(self.vect_layer)
    }
    return {**base_config, **config}

  @classmethod
  def from_config(cls, config):
    vect_layer = tf.keras.utils.deserialize_keras_object(config.pop('vect_layer'))
    return cls(vect_layer, **config)

  def call(self, x, training=False):
    x = self.vect_layer(x)
    x = self.embedding(x)
    x = self.bidirectional_1(x)
    query = self.query(x)
    value = self.value(x)
    x = self.attention([query, value])
    x = self.bidirectional_2(x)
    if training:
      x = self.dropout(x)
    x = self.dense(x)
    return x


@tf.keras.utils.register_keras_serializable()
class Arch2(tf.keras.Model):
  def __init__(self, vect_layer):
    super().__init__()
    self.vect_layer = vect_layer
    self.embedding = tf.keras.layers.Embedding(len(vect_layer.get_vocabulary()), EMBEDDING_SIZE, mask_zero=True)
    self.bidirectional_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))
    self.query = tf.keras.layers.Conv1D(200, 4, padding='same')
    self.value = tf.keras.layers.Conv1D(200, 4, padding='same')
    self.attention = tf.keras.layers.Attention(dropout=0.6)
    self.bidirectional_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))
    self.dropout = tf.keras.layers.Dropout(0.6)
    self.dense = tf.keras.layers.Dense(1, activation='sigmoid')

  def get_config(self):
    base_config = super().get_config()
    config =  {
        "vect_layer": tf.keras.utils.serialize_keras_object(self.vect_layer)
    }
    return {**base_config, **config}

  @classmethod
  def from_config(cls, config):
    vect_layer = tf.keras.utils.deserialize_keras_object(config.pop('vect_layer'))
    return cls(vect_layer, **config)

  def call(self, x, training=False):
    x = self.vect_layer(x)
    x = self.embedding(x)
    x = self.bidirectional_1(x)
    query = self.query(x)
    value = self.value(x)
    x = self.attention([query, value])
    x = self.bidirectional_2(x)
    if training:
      x = self.dropout(x)
    x = self.dense(x)
    return x

def architecture_one():
  return tf.keras.Sequential([
      vect_layer,
      tf.keras.layers.Embedding(len(vect_layer.get_vocabulary()), EMBEDDING_SIZE, mask_zero=True),
      tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
      tf.keras.layers.Dropout(0.6),
      tf.keras.layers.Dense(1, activation='tanh'),
  ])

def architecture_two():
  return tf.keras.Sequential([
      vect_layer,
      tf.keras.layers.Embedding(len(vect_layer.get_vocabulary()), EMBEDDING_SIZE, mask_zero=True),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
      tf.keras.layers.Dropout(0.6),
      tf.keras.layers.Dense(1, activation='tanh'),
  ])

# Model Creations
# We vary the optimizer between Adam and AdamW

models = [
  Arch1(vect_layer),
  Arch1(vect_layer),
  Arch2(vect_layer),
  Arch2(vect_layer),
]

models[0].compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.BinaryFocalCrossentropy(from_logits=True),
    metrics=[
        'binary_accuracy',
        tf.keras.metrics.F1Score(threshold=0.5),
      ],
    )

models[1].compile(
    optimizer=tf.keras.optimizers.AdamW(),
    loss=tf.keras.losses.BinaryFocalCrossentropy(from_logits=True),
    metrics=[
        'binary_accuracy',
        tf.keras.metrics.F1Score(threshold=0.5),
      ],
    )

models[2].compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.BinaryFocalCrossentropy(from_logits=True),
    metrics=[
        'binary_accuracy',
        tf.keras.metrics.F1Score(threshold=0.5),
      ],
    )

models[3].compile(
    optimizer=tf.keras.optimizers.AdamW(),
    loss=tf.keras.losses.BinaryFocalCrossentropy(from_logits=True),
    metrics=[
        'binary_accuracy',
        tf.keras.metrics.F1Score(threshold=0.5),
      ],
    )

# Training
histories = []

for i in range(4):
  history = models[i].fit(
      train_ds,
      epochs=100,
      validation_data=val_ds,
      callbacks=[ tf.keras.callbacks.EarlyStopping(patience=10) ]
  )

  histories.append(history)

  print("\n===== NEXT MODEL =====\n")

for model in models:
  model.evaluate(test_ds)

def plot_model(model_history, title):
  """
  Plot the training accuracy, top-5 accuracy, and loss
  for the given history object
  """

  plt.figure(figsize=(18,6))
  plt.suptitle(f"Training Results for {title}")

  plt.subplot(1, 3, 1)
  plt.title('Training and Validation Accuracy')
  plt.plot(model_history.history['binary_accuracy'], label="Training Binary Accuracy")
  plt.plot(model_history.history['val_binary_accuracy'], label="Validation Binary Accuracy")
  plt.xlabel('Epoch')
  plt.ylabel('Binary Accuracy')
  plt.legend()

  plt.subplot(1, 3, 2)
  plt.title('Training and Validation F1 Score')
  plt.plot(model_history.history['f1_score'], label="Training F1 Score")
  plt.plot(model_history.history['val_f1_score'], label="Validation F1 Score")
  plt.xlabel('Epoch')
  plt.ylabel('F1 Score')
  plt.legend()

  plt.subplot(1, 3, 3)
  plt.title('Training and Validation Loss')
  plt.plot(model_history.history['loss'], label="Training Loss")
  plt.plot(model_history.history['val_loss'], label="Validation Loss")
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

  plt.tight_layout()
  plt.show()

for idx, history in enumerate(histories):
  plot_model(history, f"Model {idx+1}")