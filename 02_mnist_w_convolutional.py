# -*- coding: utf-8 -*-
"""02_MNIST w Convolutional

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AhHIROT9XYSi5pDq1HNRwJAPJxYJB7qE

Goal: Create a model that uses convolutional layers and train it on the MNIST Cifar-10 dataset. Add code to plot a confusion matrix on the validation data.

Specifically, write code to calculate a confusion matrix of the model output on the validation data, and compare to the true labels to calculate a confusion matrix with [tf.math.confusion_matrix](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix). (For the inexperienced, [what is a confusion matrix?](https://en.wikipedia.org/wiki/Confusion_matrix)) Use the code example from [scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) to help visualise the confusion matrix if you'd like as well.
"""

# We'll start with our library imports...
import numpy as np                 # to use numpy arrays
import tensorflow as tf            # to specify and run computation graphs
import tensorflow_datasets as tfds # to load training data
import matplotlib.pyplot as plt    # to visualize data and draw plots
from tqdm import tqdm              # to track progress of loops
import seaborn as sb               # to make confusion matrices

DATA_DIR = './tensorflow-datasets/'

# Downloading and defining the train, validation, and test sets
(ds_train, ds_val, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train[:80%]', 'train[80%:]', 'test'], # 40k train, 20k val, 10k test
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

# Normalize the data
def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

train_ds = ds_train.map(preprocess).batch(64)
val_ds = ds_val.map(preprocess).batch(64)
test_ds = ds_test.map(preprocess).batch(64)

def plot_model(model_history, title):
  # Plot training history
  plt.figure(figsize=(12, 6))

  # Plot training & validation accuracy values
  plt.subplot(1, 2, 1)
  plt.plot(model_history.history['accuracy'], label='Training Accuracy')
  plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')
  plt.title(f'Training and Validation Accuracy for {title}')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()

  # Plot training & validation loss values
  plt.subplot(1, 2, 2)
  plt.plot(model_history.history['loss'], label='Training Loss')
  plt.plot(model_history.history['val_loss'], label='Validation Loss')
  plt.title('Training and Validation Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

  plt.tight_layout()
  plt.show()

# Define the models
hidden_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu, name='hidden_1')
hidden_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation=tf.nn.relu, name='hidden_2')
pool_1 = tf.keras.layers.MaxPool2D(padding='same')
hidden_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation=tf.nn.relu, name='hidden_3')
hidden_4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation=tf.nn.relu, name='hidden_4')
pool_2 = tf.keras.layers.MaxPool2D(padding='same')
flatten = tf.keras.layers.Flatten()
output = tf.keras.layers.Dense(10)

# One smaller, one bigger
model1 = tf.keras.Sequential([hidden_1, hidden_2, pool_1, flatten, output])
model2 = tf.keras.Sequential([hidden_1, hidden_2, pool_1, hidden_3, hidden_4, pool_2, flatten, output])

# Model Compilation
model1.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)
model2.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Model Training
One_history = model1.fit(
    train_ds,
    epochs=100,
    validation_data=val_ds,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=8
        )
    ],
)

print("\n===== NEXT MODEL =====\n")

Two_history = model2.fit(
    train_ds,
    epochs=100,
    validation_data=val_ds,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=8
        )
    ],
)

models = [model1, model2]
histories = [One_history, Two_history]

for idx, history in enumerate(histories):
  plot_model(history, title=f"Model {idx+1}")

test_accuracies = [model.evaluate(test_ds)[1] for model in models]
print()
for idx, test_accuracy in enumerate(test_accuracies):
  print(f'Test accuracy (Model {idx+1}):', test_accuracy)

labels = tf.concat([y for x, y in test_ds], axis=0)
(_, __, normalizing_factor) = tf.unique_with_counts(labels)
for idx, model in enumerate(models):
  preds = [tf.argmax(pred) for pred in model.predict(test_ds)]
  confusion_matrix = (
      tf.cast(tf.math.confusion_matrix(labels, preds), tf.float32)
        *
        tf.cast(1/normalizing_factor, tf.float32)
  )
  plt.figure(idx)
  plt.title(f"Confusion Matrix (Model {idx+1})")
  sb.heatmap(confusion_matrix)